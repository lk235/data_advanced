{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据质量\n",
    "# 一 有效性：审核有效性是关于确定对个别字段施加什么约束并检查以确保字段值遵守这些约束条件\n",
    "# 跨字段限制：当一个数据项拥有多个字段时，它们必须在某种程度上具有一致性\n",
    "#准确率\n",
    "#完整性\n",
    "#一致性\n",
    "#统一性\n",
    "\n",
    "#使用蓝图的示例\n",
    "#!/usr/bin/env python\n",
    "          # -*- coding: utf-8 -*-\n",
    "          import xml.etree.cElementTree as ET\n",
    "          from collections import defaultdict\n",
    "          import re\n",
    "\n",
    "          osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "          street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "          street_types = defaultdict(int)\n",
    "\n",
    "          def audit_street_type(street_types, street_name):\n",
    "              m = street_type_re.search(street_name)\n",
    "              if m:\n",
    "                  street_type = m.group()\n",
    "\n",
    "                  street_types[street_type] += 1\n",
    "\n",
    "          def print_sorted_dict(d):\n",
    "              keys = d.keys()\n",
    "              keys = sorted(keys, key=lambda s: s.lower())\n",
    "              for k in keys:\n",
    "                  v = d[k]\n",
    "                  print \"%s: %d\" % (k, v) \n",
    "\n",
    "          def is_street_name(elem):\n",
    "              return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "          def audit():\n",
    "              for event, elem in ET.iterparse(osm_file):\n",
    "                  if is_street_name(elem):\n",
    "                      audit_street_type(street_types, elem.attrib['v'])    \n",
    "              print_sorted_dict(street_types)    \n",
    "\n",
    "          if __name__ == '__main__':\n",
    "              audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习: 修正有效性\n",
    "# Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "# The following things should be done:\n",
    "# - check if the field \"productionStartYear\" contains a year\n",
    "# - check if the year is in range 1886-2014\n",
    "# - convert the value of the field to be just a year (not full datetime)\n",
    "# - the rest of the fields and values should stay the same\n",
    "# - if the value of the field is a valid year in the range as described above,\n",
    "#   write that line to the output_good file\n",
    "# - if the value of the field is not a valid year as described above, \n",
    "#   write that line to the output_bad file\n",
    "# - discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "# - you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "#   They will take care of dealing with the header.\n",
    "# 你的任务是检查 DBPedia 自动数据文件的“productionStartYear”并获取有效的值。应该完成以下任务：\n",
    "# -    检查字段“productionStartYear”是否包含年份\n",
    "# -    检查该年份是否在 1886 至 2014 范围内\n",
    "# -    将字段值转换为年份（而不是整个日期时间）\n",
    "# -    字段的其他部分和值应该保持不变\n",
    "# -    如果字段的值是如上所述范围内的有效年份，则将该行写入 output_good 文件中\n",
    "# -    如果字段的值不是如上所述的有效年份，则将该行写入 output_bad 文件中\n",
    "# -    你应该采用提供的数据读取和写入方式（DictReader 和 DictWriter），它们将会对标题进行处理。\n",
    "# \n",
    "# You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "# 'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "# 你可以编写辅助函数来检查数据并编写文件，\n",
    "# 但是我们将仅调用包含三个参数（inputfile、output_good、output_bad）的“process_file”文件。\n",
    "\n",
    "import csv\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "good = []\n",
    "bad = []\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "\n",
    "    # with open(input_file, \"r\") as f:\n",
    "    #     reader = csv.DictReader(f)\n",
    "    #     header = reader.fieldnames\n",
    "    #     print header\n",
    "    \n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        # good.append(header)\n",
    "        # bad.append(header)\n",
    "        p = re.compile('\\d{4}', re.IGNORECASE)\n",
    "        for row in reader:\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "            m =re.search('\\d{4}',row['productionStartYear'])\n",
    "            if m:\n",
    "                year = int(m.group())\n",
    "                if year > 1886 and year <  2014:\n",
    "                     row['productionStartYear'] = year\n",
    "                     good.append(row)\n",
    "                else:\n",
    "                    row['productionStartYear'] = year\n",
    "                    bad.append(row)\n",
    "                    \n",
    "            else:\n",
    "                bad.append(row)\n",
    "        \n",
    "\n",
    "        #COMPLETE THIS FUNCTION\n",
    "\n",
    "\n",
    "\n",
    "    # This is just an example on how you can use csv.DictWriter\n",
    "    # Remember that you have to output 2 files\n",
    "    \n",
    "    with open(output_good, \"w\") as g:\n",
    "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in good:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            \n",
    "    with open(output_bad, \"w\") as g:\n",
    "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in bad:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        # print datetime.strptime(row['productionStartYear'],\"%Y%d%m\")\n",
    "        \n",
    "        \n",
    "    \n",
    "test()\n",
    "\n",
    "# ANSWER:\n",
    "# def process_file(input_file, output_good, output_bad):\n",
    "#     # store data into lists for output\n",
    "#     data_good = []\n",
    "#     data_bad = []\n",
    "#     with open(input_file, \"r\") as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         header = reader.fieldnames\n",
    "#         for row in reader:\n",
    "#             # validate URI value\n",
    "#             if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "#                 continue\n",
    "# \n",
    "#             ps_year = row['productionStartYear'][:4]\n",
    "#             try: # use try/except to filter valid items\n",
    "#                 ps_year = int(ps_year)\n",
    "#                 row['productionStartYear'] = ps_year\n",
    "#                 if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "#                     data_good.append(row)\n",
    "#                 else:\n",
    "#                     data_bad.append(row)\n",
    "#             except ValueError: # non-numeric strings caught by exception\n",
    "#                 if ps_year == 'NULL':\n",
    "#                     data_bad.append(row)\n",
    "# \n",
    "#     # Write processed data to output files\n",
    "#     with open(output_good, \"w\") as good:\n",
    "#         writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "#         writer.writeheader()\n",
    "#         for row in data_good:\n",
    "#             writer.writerow(row)\n",
    "# \n",
    "#     with open(output_bad, \"w\") as bad:\n",
    "#         writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "#         writer.writeheader()\n",
    "#         for row in data_bad:\n",
    "#             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['URI', 'rdf-schema#label', 'rdf-schema#comment', 'administrativeDistrict_label', 'administrativeDistrict', 'anthem_label', 'anthem', 'area', 'areaCode', 'areaLand', 'areaMetro', 'areaRural', 'areaTotal', 'areaUrban', 'areaWater', 'city_label', 'city', 'code', 'country_label', 'country', 'daylightSavingTimeZone_label', 'daylightSavingTimeZone', 'district_label', 'district', 'division_label', 'division', 'elevation', 'federalState_label', 'federalState', 'foundingDate', 'foundingPerson_label', 'foundingPerson', 'foundingYear', 'governingBody_label', 'governingBody', 'government_label', 'government', 'governmentType_label', 'governmentType', 'isPartOf_label', 'isPartOf', 'isoCodeRegion_label', 'isoCodeRegion', 'leader_label', 'leader', 'leaderName_label', 'leaderName', 'leaderParty_label', 'leaderParty', 'leaderTitle', 'location_label', 'location', 'maximumElevation', 'mayor_label', 'mayor', 'minimumElevation', 'motto', 'municipality_label', 'municipality', 'part_label', 'part', 'percentageOfAreaWater', 'populationAsOf', 'populationDensity', 'populationMetro', 'populationMetroDensity', 'populationRural', 'populationTotal', 'populationTotalRanking', 'populationUrban', 'populationUrbanDensity', 'postalCode', 'region_label', 'region', 'state_label', 'state', 'synonym', 'thumbnail_label', 'thumbnail', 'timeZone_label', 'timeZone', 'twinCity_label', 'twinCity', 'twinCountry_label', 'twinCountry', 'type_label', 'type', 'utcOffset', 'point', '22-rdf-syntax-ns#type_label', '22-rdf-syntax-ns#type', 'wgs84_pos#lat', 'wgs84_pos#long', 'depiction_label', 'depiction', 'homepage_label', 'homepage', 'name', 'nick']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'areaLand'",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-449a751a7e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m#     test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0maudit_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCITIES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFIELDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-449a751a7e85>\u001b[0m in \u001b[0;36maudit_file\u001b[0;34m(filename, fields)\u001b[0m\n\u001b[1;32m    117\u001b[0m                  \u001b[0mfieldtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mis_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                  \u001b[0mfieldtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mis_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                  \u001b[0mfieldtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'areaLand'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#练习: 审查数据质量\n",
    "# In this problem set you work with cities infobox data, audit it, come up with a\n",
    "# cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "# the datatypes that can be found in some particular fields in the dataset.\n",
    "# The possible types of values can be:\n",
    "# - NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "# - list, if the value starts with \"{\"\n",
    "# - int, if the value can be cast to int\n",
    "# - float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "#    For example, '3.23e+07' should be considered a float because it can be cast\n",
    "#    as float but int('3.23e+07') will throw a ValueError\n",
    "# - 'str', for all other values\n",
    "# 在此习题集中，你将处理城市 infobox 数据，对数据进行审核，然后想出清理方法并清理数据。在第一道练习中，请审核\n",
    "# 数据集中某些特定字段中的数据类型。\n",
    "# 值类型可以是：\n",
    "# -    NoneType，如果值是字符串“NULL”或空字符串“”\n",
    "# -    列表，如果值以“{”开头\n",
    "# -    整型，如果值可以转型为整型\n",
    "# -    浮点型，如果值可以转型为浮点型，但是无法转型为整型。\n",
    "# 例如，“3.23e+07”应该被当做浮点型，因为可以转型为浮点型，但是int('3.23e+07') 将抛出 ValueError\n",
    "# -    “str”，表示其他所有值\n",
    "# \n",
    "# The audit_file function should return a dictionary containing fieldnames and a \n",
    "# SET of the types that can be found in the field. e.g.\n",
    "# {\"field1\": set([type(float()), type(int()), type(str())]),\n",
    "#  \"field2\": set([type(str())]),\n",
    "#   ....\n",
    "# }\n",
    "# The type() function returns a type object describing the argument given to the \n",
    "# function. You can also use examples of objects to create type objects, e.g.\n",
    "# type(1.1) for a float: see the test function below for examples.\n",
    "# audit_file 函数应该返回一个字典，其中包含字段名称和可以在该字段中找到的类型集。例如\n",
    "# {\"field1\": set([type(float()), type(int()), type(str())]),\n",
    "#  \"field2\": set([type(str())]),\n",
    "#   ....\n",
    "# }\n",
    "# type() 函数返回的是类型对象，描述了提供给该函数的参数。你还可以使用对象示例创建类型对象，例如 type(1.1) \n",
    "# 表示浮点型：具体示例请参阅下面的测试函数。\n",
    "# \n",
    "# Note that the first three rows (after the header row) in the cities.csv file\n",
    "# are not actual data points. The contents of these rows should note be included\n",
    "# when processing data types. Be sure to include functionality in your code to\n",
    "# skip over or detect these rows.\n",
    "# 注意，cities.csv 文件的前三行（标题行之后）不是实际的数据点。在处理数据类型时，不应该包含这些行的内容。\n",
    "# 确保在代码中包含相关功能，以便跳过或检测出这些行。\n",
    "\n",
    "# <type 'float'>\n",
    "# <type 'str'>\n",
    "# <type 'int'>\n",
    "# <type 'str'>\n",
    "# <type 'NoneType'>\n",
    "\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "FIELDS = [\"areaLand\", \"areaMetro\"]\n",
    "# FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "#           \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "#           \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "#           \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def skip_lines(input_file, skip):\n",
    "    for i in range(0,skip):\n",
    "        next(input_file)\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_null(s):\n",
    "    try:\n",
    "        s == None or s == ''\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_list(s):\n",
    "    try:\n",
    "        # s.startwith('{')\n",
    "        s[0] == '{'\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "    \n",
    "def audit_file(filename, fields):\n",
    "    \n",
    "    fieldtypes = {}\n",
    "   \n",
    "    with open(filename,'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        print header\n",
    "        skip_lines(reader,3)\n",
    "    \n",
    "    # for field in fields:\n",
    "    #     fieldtypes[field] = set()\n",
    "        for row in reader:\n",
    "            for field in fields:\n",
    "                if is_float(row[field]):\n",
    "                 fieldtypes[field].add(type(float()))\n",
    "                elif is_int(row[field]):\n",
    "                 fieldtypes[field].add(type(int()))\n",
    "                # elif is_list(row[field]):\n",
    "                #  fieldtypes[field].add(type(list()))\n",
    "                elif is_null(row[field]):\n",
    "                 fieldtypes[field].add(type(None))\n",
    "                else:\n",
    "                 fieldtypes[field].add(type(str()))\n",
    "         \n",
    "            \n",
    "    # print header\n",
    "        \n",
    "            \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "    \n",
    "# def test():\n",
    "#     fieldtypes = audit_file(CITIES, FIELDS)\n",
    "# \n",
    "#     pprint.pprint(fieldtypes)\n",
    "# \n",
    "#     assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "#     assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "#     \n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n",
    "\n",
    "audit_file(CITIES,FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n<type 'str'>\n<type 'int'>\n<type 'list'>\n<type 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print type(float())\n",
    "print type('SSS')\n",
    "print type(int())\n",
    "print type(list())\n",
    "print type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
